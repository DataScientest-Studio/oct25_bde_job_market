Project Name
==============================

## Overview

This project predicts job market salaries using machine learning and provides a web interface for interactive predictions. It ingests job data from the Adzuna API, stores it in PostgreSQL and MongoDB, trains a RandomForest model, and exposes predictions via both a REST API and Streamlit web UI.

**Key Features:**
- ğŸ”„ Automated daily job data ingestion (988+ jobs currently)
- ğŸ¤– ML-powered salary predictions (MAE: â‚¬13,935)
- ğŸŒ REST API for programmatic access
- ğŸ’» Interactive Streamlit dashboard
- ğŸ“Š Cross-platform Docker setup (Mac/Windows/Linux)

---

## Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+ (for local development)

### Setup & Run

```bash
# Clone and navigate to project
cd oct25_bde_job_market

# Copy environment template
cp .env.example .env

# Start all services (PostgreSQL, MongoDB, FastAPI, Streamlit)
docker-compose up -d

# Services will be available at:
# - API: http://localhost:8000/docs (Swagger UI)
# - Streamlit: http://localhost:8501
# - pgAdmin: http://localhost:5050
# - Mongo Express: http://localhost:8081
```

### Using the Salary Predictor

**Via Streamlit UI:**
1. Open http://localhost:8501
2. Enter job details (title, description, city, etc.)
3. Click "Predict Salary" to get an instant prediction

**Via API (POST):**
```bash
curl -X POST "http://localhost:8000/ml/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Senior Python Developer",
    "job_description": "Build scalable backend systems",
    "city": "Berlin",
    "country": "Deutschland",
    "contract_type": "permanent",
    "contract_time": "full_time"
  }'
```

---

## Project Organization


    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ logs               <- Logs from training and predicting
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â  â”‚   â””â”€â”€ visualize.py
    â”‚Â Â  â””â”€â”€ config         <- Describe the parameters used in train_model.py and predict_model.py

--------

## Architecture & Components

### Current Services
- **FastAPI** (Port 8000): REST API for salary predictions and data endpoints
- **PostgreSQL**: Relational database for structured job data
- **MongoDB**: NoSQL database for job descriptions and metadata
- **Streamlit** (Port 8501): Interactive UI for salary predictions
- **ML Model**: RandomForest regressor trained on historical salary data

### Missing Component: Apache Airflow
**Status:** â³ To be implemented

Airflow will automate the daily workflows for:
- **Data Ingestion**: Fetch latest job postings from Adzuna API and sync to PostgreSQL/MongoDB
- **Model Retraining**: Retrain the salary prediction model with newly ingested data
- **Monitoring**: Track data quality and model performance metrics

**Proposed Setup:**
```yaml
airflow:
  image: apache/airflow:latest
  services:
    - scheduler: Orchestrates DAG execution
    - webserver: Monitoring & management UI (Port 8080)
    - executor: Celery or Local executor for task distribution
```

**DAGs to implement:**
1. `daily_ingestion_dag`: Run `/data` endpoint to fetch new jobs (triggers at 02:00 UTC)
2. `weekly_training_dag`: Retrain model if sufficient new salary data exists (triggers Sundays at 03:00 UTC)
3. `data_quality_dag`: Validate data integrity and alert on anomalies

--------

